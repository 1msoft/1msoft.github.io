# 前言

## 编写目的

通过编写此文档，Serverless项目团队成员可在对kubernetes容器集群管理平台、Knative Serverless架构方案仅有基础的认知的情况下，快速在英迈软件搭建的Serverless架构平台上快速构建、部署、管理应用。

## 阅读对象

福建英迈软件Serverless项目团队涉及的产品经理、项目经理、研发工程师，系统运维工程师，包括公司内部员工及外部合作伙伴。

## 使用说明

通过本手册学习如何在福建英迈软件Serverless架构平台上快速构建、部署、管理应用，并结合《福建英迈软件Knative事件流设计最佳实践手册》学习如何基于将Knative事件流应用到实际项目当中。

## 版本说明

2020-12-31  发布新建版本

# 架构方案介绍

## Serverless简介

Serverless 是一种架构方案，目标是让开发者可以将开发重点关注到更有价值的业务代码，而不需要关注运行应用程序所需要的服务器等基础设施，将构建应用的成本进一步降低。此外，函数完全由事件触发，平台根据请求自动平行调整服务资源，拥有近乎无限的扩容能力，空闲时则没有任何资源在运行，这一特性让用户可以只为实际使用的资源付费。函数运行无状态，可以更加简单的实现快速迭代、极速部署。

## Kubernetes简介

[**Kubernetes**](https://www.kubernetes.org.cn/)是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。

Kubernetes一个核心的特点就是能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行着，管理员可以加载一个微型服务，让规划器来找到合适的位置，同时，Kubernetes也系统提升工具以及人性化方面，让用户能够方便的部署自己的应用。Kubernetes整体架构如下图：

- ![avatar](.\images\Kubenetes架构.jpg)

Kubernetes是master-agent架构，master是Kubernetes的运行管理中心，node运行应用程序的节点。

+ master上运行着以下核心组件：
    + kube-apiserver：整个Kubernetes的调度中心，通过暴露rest接口, 接受来自其他组件的请求并且与持久化存储(etcd)交互
    + kube-scheduler：调度进程，支持多种算法，为pod找到适合的node
    + kube-controller-manager：主要的守护进程，管理着所有的control loop, 通过监听api-server，使各个类型的资源达到预期的状态。
    + etcd：提供k8s的数据存储服务，是一个强一致性的内存数据库系统，存储配置以及资源对象。

+ node上运行着以下核心组件：

    + kubelet：监听着apiserver的接口，管理需要在当前节点运行的pod

    + kube-proxy：管理各个节点上的网络规则，实现了service的流量转发以及负载均衡。支持userspace(deprecated)、iptables以及ipvs(推荐使用)

    + container runtime engine：提供了容器运行时的接口，支持多种容器实现，比如docker、rkt等。

+ pod：k8s可以创建和管理的最小单元。一个pod可以有多个容器。不具有持久化的特性。pod内的容器共享网络空间和存储，可以认为是一个逻辑主机。

## Istio简介

随着微服务的广泛应用，微服务的连接、管理和监控成为了难题。 Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。

Istio在服务网络上统一提供了许多关键功能:

- 流量管理(Pilot)：控制服务之间的流量和API调用的流向，使得调用更灵活可靠，并使网络在恶劣情况下更加健壮。
- 可观察性：过集成zipkin等服务，快速了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。
- 策略执行(mixer)：将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。策略的更改是通过配置网格而不是修改应用程序代码。
- 服务身份和安全(Istio-auth)：为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。

## Knative简介

Knative 是 Google 在 2018 的 Google Cloud Next 大会上发布的一款基于 Kubernetes 的 Serverless 框架。Knative 一个很重要的目标就是制定云原生、跨平台的 Serverless 编排标准。Knative 是通过整合容器构建(或者函数)、工作负载管理(和动态扩缩)以及事件模型这三者来实现的这一 Serverless 标准。Knative 将重点放在三个关键组件上：build（构建）你的应用程序，为其提供流量serving（服务），以及确保应用程序能够轻松地生产和消费event（事件）：

+ build（构建）：通过灵活的插件化的构建系统将用户源代码构建成容器。目前已经支持多个构建系统，比如 Google 的 Kaniko，它无需运行 Docker daemon 就可以在 Kubernetes 集群上构建容器镜像。随着越来越多的功能加入到build中，build模块越来越变得像一个通用的CI/CD系统，这已经脱离了Knative build设计的初衷，于是将build-pipeline剥离出Knative，摇身一变成为Tekton，而Tekton也从此致力于提供全功能、标准化的原生kubernetesCI/CD解决方案。
+ Serving（服务）：基于负载自动伸缩，包括在没有负载时缩减到零。允许你为多个修订版本（revision）应用创建流量策略，从而能够通过 URL 轻松路由到目标应用程序。
+ Event（事件）：使得生产和消费事件变得容易。抽象出事件源，并允许操作人员使用自己选择的消息传递层。

knative 是建立在 kubernetes 和 Istio 平台之上的，Knative 一方面基于 Kubernetes 实现 Serverless 编排，另外一方面 Knative 还基于 Istio 实现服务的接入、服务路由的管理以及度发布等功能，各角色之间的协作关系如下图：
![avatar](.\images\knative.png)

+ Developers  ：Serverless 服务的开发人员可以直接使用原生的 Kubernetes API 基于

Knative 部署 Serverless 服务快速入门

+ Contributors：主要是指社区的贡献者

+ Operators  ：Knative 可以被集成到任何支持的环境中，比如：云厂商、或者企业内部。目前 Knative 是基于 Kubernetes 来实现的，有 Kubernetes 的地方就可以部署 Knative

+ Users：终端用户通过 Istio 网关访问服务，或者通过事件系统触发 Knative 中的Serverless 服务

Knative 是以 Kubernetes 的一组自定义资源类型（CRD）的方式来安装的，因此只需使用几个 YAML 文件就可以轻松地开始使用 Knative 了。这也意味着，在本地或者托管云服务上，任何可以运行 Kubernetes 的地方都可以运行 Knative 和你的代码。

# 部署前提

由于搭建基于Knative的Serverless平台具有一定的难度，且部署一套就可供多个项目在此平台上构建应用，Serverless项目团队成员并不需要各个项目组自行搭建，仅需在已搭建好的平台上构建、部署、管理应用。

可在mater节点上执行以下命令查看Kubernetes当前的部署情况：
```shell
kubectl get nodes
```

输出结果：
```shell
NAME        STATUS   ROLES    AGE   VERSION
kubenode1   Ready    <none>   16d   v1.18.1
kubenode3   Ready    master   16d   v1.18.1
```

通过以上输出结果可以看到当前部署了两台服务器，一台为master，一台为node,均处于ready状态。

可在master节点上执行以下命令查看当前所有的pod：

```shell
kubectl get pods -n istio-system
```

部分输出结果：

```
NAMESPACE          NAME                                                              READY   STATUS      RESTARTS   AGE
istio-system       grafana-54485b765d-lt89k                                          1/1     Running     0          8d
istio-system       istio-citadel-7c686c649d-7hr8t                                    1/1     Running     0          8d
istio-system       istio-egressgateway-6947dc8447-8nkjq                              1/1     Running     0          8d
istio-system       istio-galley-5487b79c-kzs65                                       1/1     Running     0          8d
istio-system       istio-ingressgateway-866df464c7-2fvx2                             1/1     Running     0          8d
istio-system       istio-pilot-d88ffd4df-62j2p                                       2/2     Running     0          8d
istio-system       istio-policy-67b566c4ff-rkl86                                     2/2     Running     0          8d
istio-system       istio-sidecar-injector-688d5dfbdc-fc282                           1/1     Running     0          8d
istio-system       istio-telemetry-6fffdcb64d-l9542                                  2/2     Running     0          8d
istio-system       istio-tracing-b47fc45db-7gsq2                                     1/1     Running     9          8d
istio-system       kiali-65d9c744cf-5nknw                                            1/1     Running     0          8d
istio-system       prometheus-68cd967d87-bqhl9                                       1/1     Running     0          8d

```

通过以上输出结果可以看到当前Kubernetes集群上已安装Istio且服务正常运行。

可在master节点上执行以下命令查看当前所有的pod：

```shell
kubectl get pods --all-namespaces
```

部分输出结果：

```
knative-eventing   eventing-controller-569b7f9597-d4slx                              1/1     Running     0          8d
knative-eventing   eventing-webhook-6f85c4c447-n8spm                                 1/1     Running     0          8d
knative-eventing   imc-controller-9957b4d74-67z5x                                    1/1     Running     0          3d17h
knative-eventing   imc-dispatcher-c77df98f4-xjz84                                    1/1     Running     0          3d17h
knative-eventing   mt-broker-controller-574b4667dd-wdvvj                             1/1     Running     0          3d18h
knative-eventing   mt-broker-filter-79c59cc4dc-ljtcv                                 1/1     Running     0          3d18h
knative-eventing   mt-broker-ingress-c44b7bd89-s84rt                                 1/1     Running     0          3d18h
knative-serving    activator-6875896748-nhqg8                                        1/1     Running     0          8d
knative-serving    autoscaler-6bbc885cfd-qcvwk                                       1/1     Running     0          8d
knative-serving    controller-64dd4bd56-6skj2                                        1/1     Running     0          8d
knative-serving    istio-webhook-75cc84fbd4-t6pvx                                    1/1     Running     0          8d
knative-serving    networking-istio-6dcbd4b5f4-sggtq                                 1/1     Running     0          8d
knative-serving    webhook-75f5d4845d-qg7fq                                          1/1     Running     0          8d
tekton-pipelines   import-resources-1609210779395-import-resources-mb9vd-pod-9frc7   0/2     Completed   0          5d23h
tekton-pipelines   tekton-dashboard-59cdc76944-l8kfh                                 1/1     Running     0          6d16h
tekton-pipelines   tekton-pipelines-controller-f5945cf7b-dfgtm                       1/1     Running     0          6d16h
tekton-pipelines   tekton-pipelines-webhook-859cd5c766-ztnvr                         1/1     Running     0          6d16h


```

通过以上输出结果可以看到当前Kubernetes集群上已安装knative的serving、eventing、tekton等资源。

# 开发指导

下文将介绍knative的serving、eventing组件以及tekton框架，并以简单的hello world程序介绍它们的用法。

## Serving组件

 Knative Serving 构建于 Kubernetes 和 Istio 之上，为 Serverless 应用提供部署和服务支持。其特性如下：

- 快速部署 Serverless 容器
- 支持自动扩缩容和缩到 0 实例
- 基于 Istio 组件，提供路由和网络编程

Knative  Serving需要将不同的语言应用程序编译镜像Dockerfile 并且使用线上docker仓库，通过镜像地址，下载镜像并安装到集群中，完成部署。部署的服务将会自动扩缩容，当没有使用服务的时候，服务会自动停止服务并

通过编写配置文件执行部署。Serving 模块定义以下对象以控制所有功能：

- Service: 自动管理工作负载整个生命周期。负责创建 Route、Configuration 以及 Revision 资源。通过 Service 可以指定路由流量使用最新的 Revision 还是固定的 Revision
- Route：负责映射网络端点到一个或多个 Revision。可以通过多种方式管理流量。包括灰度流量和重命名路由
- Configuration: 负责保持 Deployment 的期望状态，提供了代码和配置之间清晰的分离，并遵循应用开发的 12 要素。修改一次 Configuration 产生一个 Revision
- Revision：Revision 资源是对工作负载进行的每个修改的代码和配置的时间点快照。Revision 是不可变对象，可以长期保留

### 创建服务

Knative 官方给出了好几种语言的 Helloworld 示例，这些不同的语言其实只是编译镜像的 Dockerfile 有所不同，做好镜像之后的使用方式没什么差异。本例以 go 的 Hello World 为例进行演示。官方给出的例子都是源码，需要编译长镜像才能使用。为了验证方便使用阿里云上别人已经提前编译好了一份镜像 registry.cn-hangzhou.aliyuncs.com/knative-sample/helloworld-go:160e4dc8 , 可以直接使用。

首先编写一个 Knative Service 的 yaml 文件 helloworld-go.yaml , 内容如下：

```yaml
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: helloworld-go
spec:
  template:
    metadata:
      labels:
        app: helloworld-go
      annotations:
        autoscaling.knative.dev/target: "10"
    spec:
      containers:
        - image: registry.cn-hangzhou.aliyuncs.com/knative-sample/helloworld-go:160e4dc8
          ports:
            - name: http1
              containerPort: 8080
          env:
            - name: TARGET
              value: "hello World"
```

自行部署服务时应修改以下配置项：	

+ name：修改为自定义的服务名称

+ app：修改为自定义的app名称

+ image：修改为自定义的镜像地址

+ containerPort：为访问自定义服务的端口号

+ autoscaling.knative.dev/target：当前设置为"10",指每一个 Pod 的可处理并发请求数 10 ，Knative KPA 自动伸缩的时候会根据当前总请求的并发数和 autoscaling.knative.dev/target 自动调整 Pod 的数量，从而达到自动扩缩的目的，可自行修改。

+ 如果需要添加环境变量，可在env下面添加

使用 kubectl 命令把 yaml 提交到 Kubernetes 中 :

  ```shell
kubectl apply -f helloworld-go.yaml
  ```

可以查看到服务部署成功:
```shell
service.serving.knative.dev/helloworld-go created
```

可以使用以下命名查看pod:

```shell
kubectl get pods
```
可以查看到该服务已启动（长时间未请求会自动缩容为0，则使用该命令看不到该服务）:
```shell
NAME                                                     READY   STATUS               RESTARTS   AGE
helloworld-go-00001-deployment-7959595b-xdx4m            2/3     Running              0          8s
```

### 查询服务

可使用以下命令查询服务：

```shell
kubectl get svc -n “命名空间”
```

服务创建时未指定命名空间，则默认在default命名空间下，使用kubectl get svc -n default命令可查看到如下内容：

```shell
NAME            URL                                        LATESTCREATED         LATESTREADY           READY   REASON
helloworld-go   http://helloworld-go.default.example.com   helloworld-go-00001   helloworld-go-00001   True  
```

### 访问服务

所以想要访问 Knative 的服务首先要获取 Gateway 的 IP 地址，可以通过如下方式获取 Gateway 的 IP：

```
kubectl get svc istio-ingressgateway --namespace istio-system --output jsonpath="{.status.loadBalancer.ingress[*].ip}"
```

输出示例

```
192.168.31.199
```

Gateway 是通过 VirtualService 来进行流量转发的，这就要求访问者要知道目标服务的名字才行 ( 域名 )，所以要先获取 helloworld-go 的域名 ,注意下面这条命令中的 ${SVC_NAME} 需要替换成 helloworld-go ，这个名字必须要和 Knative Service 的名字一致，因为每一个 Service 都有一个唯一的名字。

```shell
kubectl get route ${SVC_NAME} --output jsonpath="{.status.domain}"
```

输出示例

```
helloworld-go.default.example.com
```

至此你已经拿到 IP 地址和 Hostname，可以通过 curl 直接发起请求：

```
curl -H "Host: helloworld-go.default.example.com" "http://192.168.31.199"
```

也可以使用Postman工具进行测试访问，步骤如下：

+ 在地址栏中输入网关的ip或域名 192.168.31.199

+ 在headers中添加请求头Host，值为helloworld-go.default.example.com

+ 点击发送
即可以发送请求至该服务并接收到响应内容“Hello World!”。



## Eventing组件

基于事件驱动是 Serverless 的核心功能之一，通过事件驱动服务，满足了用户按需付费（Pay-as-you-go）的需求。 Knative Eventing 由事件源、事件处理模型和事件消费 3 个主要部分构成，Knative Eventing通过外部事件触发服务，通过Eventing中的事件源来接收外部事件的信号， 定义 Broker 和 Trigger 对象，实现了对事件进行过滤（亦如通过 ingress 和 ingress controller 对网络流量的过滤一样）。 通过定义 Broker 创建 Channel，通过 Trigger 创建 Channel 的订阅（subscription），并产生事件过滤规则，最终启动serving中部署的服务完成事件消费。
当前支持以下几种类型的事件源：

- ApiserverSource：每次创建或更新 Kubernetes 资源时，ApiserverSource 都会触发一个新事件
- GitHubSource：GitHub 操作时，GitHubSource 会触发一个新事件
- GcpPubSubSource： GCP 云平台 Pub/Sub 服务会触发一个新事件
- AwsSqsSource：Aws 云平台 SQS 服务会触发一个新事件
- ContainerSource: ContainerSource 将实例化一个容器，通过该容器产生事件
- PingSource: 通过 CronJob 产生事件
- KafkaSource: 接收 Kafka 事件并触发一个新事件
- CamelSource: 接收 Camel 相关组件事件并触发一个新事件
- RabbitmqSource：接收 Rabbitmq 事件并触发一个新事件


当前支持如下事件接收处理：

+ 直接事件接收
  通过事件源直接转发到单一事件消费者。支持直接调用 Knative Service 或者
Kubernetes Service 进行消费处理。这样的场景下，如果调用的服务不可用，
事件源负责重试机制处理。
+ 通过事件通道 (Channel) 以及事件订阅 (Subscriptions) 转发事件处理
  这样的情况下，可以通过 Channel 保证事件不丢失并进行缓冲处理，通过
Subscriptions 订阅事件以满足多个消费端处理。
+ 通过 brokers 和 triggers 支持事件消费及过滤机制


为了满足将事件发送到不同类型的服务进行消费，Knative Eventing 通过多个k8s 资源定义了两个通用的接口：

+ Addressable 接口提供可用于事件接收和发送的 HTTP 请求地址，并通过
  status.address.hostname 字 段 定 义。 作 为 一 种 特 殊 情 况，Kubernetes
  Service 对象也可以实现 Addressable 接口

+ Callable 接口接收通过 HTTP 传递的事件并转换事件。可以按照处理来自外
  部事件源事件的相同方式，对这些返回的事件做进一步处理

当前 Knative 支持通过 Knative Service 或者 Kubernetes Service 进行消费事件。

### 创建事件源

下面分别以RabbitmqSource和PingSource作为事件源，直接接收事件和通过broker/trigger的方式触发服务两种方式为例。

以RabbitmqSource作为事件源需添加对应rabbitmq服务的账号密码在所选的命名空间下,可采用下列命令添加：

```shell
kubectl create secret generic rabbitmq-source-key --from-literal=user=guest --from-literal=password=guest
```

创建event-source.yaml文件，通过sink配置指定事件的接受方，service表示直接触发服务，broker表示通过broker触发，下面列举几种常用的类型的文件配置。

RabbitmqSource事件源直接触发服务：

```yaml
apiVersion: sources.knative.dev/v1alpha1
kind: RabbitmqSource
metadata:
  name: rabbitmq-source
spec:
  brokers: "192.168.31.246:5672/"
  topic: ""
  user:
    secretKeyRef:
      name: "rabbitmq-source-key"
      key: "user"
  password:
    secretKeyRef:
      name: "rabbitmq-source-key"
      key: "password"
  exchange_config:
    name: "logs"
    type: "fanout"
    durable: false
    auto_deleted: false
    internal: false
    nowait: false
  queue_config:
    name: "kantive-test"
    routing_key: ""
    durable: true
    delete_when_unused: false
    exclusive: false
    nowait: false
  sink:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: helloworld-go
```

PingSource直接触发服务, 该配置文件定义每分钟输出一次“Hello world!”：

```yaml
kind: PingSource
metadata:
  name: test-ping-source
spec:
  schedule: "*/1 * * * *"
  jsonData: '{"message": "Hello world!"}'
  sink:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: helloworld-go
```

PingSource通过broker/trigger的方式触发服务，该配置文件定义每分钟输出一次“Hello world!”：

```yaml
apiVersion: sources.knative.dev/v1alpha2
kind: PingSource
metadata:
  name: test-ping-source
spec:
  schedule: "*/1 * * * *"
  jsonData: '{"message": "Hello world!"}'
  sink:
    ref:
      # Deliver events to Broker.
      apiVersion: eventing.knative.dev/v1
      kind: Broker
      name: default
```

使用以下命令创建事件源：

```shell
kubectl apply -f contrib/rabbitmq/samples/event-source.yaml
```

通过broker/trigger的方式触发服务，则需要创建broker和trigger, 直接触发服务则不需要。

### 创建broker

在所选命名空间下，创建 `default` Broker。假如选择 `default` 命名空间， 执行如下命令：

```shell
kubectl label namespace default knative-eventing-injection=enabled
```

这里 Eventing Controller 会根据设置`knative-eventing-injection=enabled` 标签的 namepace， 自动创建 Broker。并且使用在webhook中默认配置的 ClusterChannelProvisioner（in-memory）。

目前除了系统自身支持的基于内存的消息通道 InMemoryChannel 之外，还支持 Kafka、NATS Streaming 等消息服务（KafkaChannel）。下面以InMemoryChannel 为例，创建broker.yaml文件：

```yaml
apiVersion: eventing.knative.dev/v1
kind: Broker
metadata:
  name: default
spec:
  channelTemplateSpec:
    apiVersion: messaging.knative.dev/v1alpha1
    kind: InMemoryChannel
```
使用以下命令创建broker:
```shell
kubectl apply --filename broker.yaml
```

### 创建trigger

这里为默认的 Broker 创建一个最简单的 Trigger，并且使用 Service 进行订阅。trigger.yaml 示例如下：

```yaml
apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: my-service-trigger
spec:
  broker: default
  filter:
    attributes:
      type: dev.knative.foo.bar
      myextension: my-extension-value
  subscriber:
    ref:
      apiVersion: serving.knative.dev/v1
      kind: Service
      name: my-service
```

使用以下命令创建trigger:

```shell
kubectl apply --filename trigger.yaml
```

### 触发事件

在上面的两个例子中，PingSource事件源每分钟会发出消息，对应的Service服务被触发。测试RabbitmqSource可在Rabbitmq客户端的队列中发送消息，使用时需要注意发送的数据必须是json格式的否则会出现问题，事件源接收到消息即可触发对应的service服务。

注意：如果发送请求给服务失败，可能是istio的网关服务的问题，使用以下命令查看内部访问网关的网关名：

```shell
kubectl get svc -n istio-system
```

输出结果：

```shell
NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                                                      AGE
knative-local-gateway   ClusterIP      10.96.66.250    <none>        80/TCP,443/TCP,31400/TCP,15011/TCP,8060/TCP,15029/TCP,15030/TCP,15031/TCP,15032/TCP                                                          40m
istio-ingressgateway    LoadBalancer   10.96.253.243   <pending>     15020:32433/TCP,80:31380/TCP,443:31390/TCP
```

通过以上输出结果可看出网关名称为knative-local-gateway，通过以下命令检查Istio的网关配置：

```shell
kubectl edit configmap config-istio -n knative-serving
```

查看local-gateway.knative-serving.cluster-local-gateway配置项的值是否与网关名称对应，默认情况下是cluster-local-gateway.istio-system.svc.cluster.local，应根据查询结果修改成knative-local-gateway.istio-system.svc.cluster.local。如下所示：

```shell
local-gateway.knative-serving.cluster-local-gateway: "knative-local-gateway.istio-system.svc.cluster.local"
```

## Tekton框架

Tekton 是一个功能强大且灵活的 Kubernetes 原生 CI/CD 开源框架。通过抽象底层实现细节，用户可以跨多云平台和本地系统进行构建、测试和部署。具有组件化、声明式、可复用及云原生的特点。Tekton 作为 Knative Build 模块的升级版，提供了更丰富的功能，可以适用更多的场景，主要由如下五个核心概念组成：

+ Task：Task 就是定义一个要执行的任务，可以是编译打包，可以是测试单元，也可以是生成镜像。

+ TaskRun：Task 定义好以后是不能执行的，就像一个函数定义好以后需要调用才能执行一样，taskRun才真正代表了一次实际的运行。

+ Pipeline： 一个 TaskRun 只能执行一个 Task，有时候需要执行多个Task，那么需要流水线执行多个Task，与jenkins的流水线差不多。

+ PipelineRun： 和 Task 一样 Pipeline 定义完成以后也是不能直接执行的，需要run来执行。

+ PipelineResource：Task 之间共享资源，一般是定义代码仓库的资源，可以直接设置github地址。

基本流程如下：

```sequence
participant PipelineResource
participant Task
participant Pipeline
participant PipelineRun
participant knative



note over PipelineResource: 创建github资源
note over PipelineResource: 创建github的秘钥配置
note over PipelineResource: 创建docker仓库的秘钥配置
note over Task: 创建编译并生成镜像的任务
note over Task: 创建部署到knative的任务
Task  -> Pipeline: 添加到流水线中
Pipeline -> PipelineRun: 配置启动流水线
PipelineResource  -> PipelineRun: 将资源配置到启动流水线配置中
note over PipelineRun: 执行流水线完成流程
PipelineRun  -> knative: 上线成功

```

流水线完成后在启动流水线的配置中，只需要输入对应的变量就可以完成多个项目使用相同一个流水线部署的功能。下面使用[官网的例子](https://github.com/knative-sample/tekton-knative/tree/b1.0)体验使用 Tekton 从源码到构建再到部署的自动化过程。克隆代码到本地，切换到 b1.0 分支，到 tekton-cicd 目录进行后面的操作。

### 创建 PipelineResource
主要内容在 resources/picalc-git.yaml 文件中。如下所示主要是把 https://github.com/knative-sample/tekton-knative/tree/b1.0 保存在 resource 中给其他资源使用。

```yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
 name: tekton-knative-git
spec:
 type: git
 params:
 - name: revision
   value: b1.0
 - name: url
   value: https://github.com/knative-sample/tekton-knative
```

### 创建 task

创建 task，这个例子中我们创建两个 task：source-to-image 和 deploy-us￾ing-kubectl。

 source-to-image主要内容在 tasks/source-to-image.yaml 文件中。此 task 的主要功能是把源代码编译成镜像。主要是使用 kaniko 实现容器内编译 Docker 镜像的能力。此 Task 的参数主要是设置编译上下文的一些信息，比如：Dockerfile、ContextPath 以及目标镜像 tag 等。

```yaml
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
 name: source-to-image
spec:
 inputs:
 resources:
 - name: git-source
   type: git
    params:
 - name: pathToContext
   description: The path to the build context, used by Kaniko - within 
   the workspace
    default: .
 - name: pathToDockerFile
   description: The path to the dockerfile to build (relative to the 
   context)
    default: Dockerfile
 - name: imageUrl
   description: Url of image repository
 - name: imageTag
   description: Tag to apply to the built image
    default: "latest"
    steps:
 - name: build-and-push
   image: registry.cn-hangzhou.aliyuncs.com/knative-sample/kaniko-project￾executor:v0.10.0
    command:
 - /kaniko/executor
   args:
 - --dockerfile=$(inputs.params.pathToDockerFile)
 - --destination=$(inputs.params.imageUrl):$(inputs.params.imageTag)
 - --context=/workspace/git-source/$(inputs.params.pathToContext)
   env:
 - name: DOCKER_CONFIG
   value: /builder/home/.docker
```


 deploy-using-kubectl主要内容在 tasks/deploy-using-kubectl.yaml 文件中。如下所示这个 Task 主要的作用是通过参数获取到目标镜像的信息，然后执行一条 sed 命令把 Knative Service yaml 中的 __IMAGE__ 替换成目标镜像。再通过kubectl 发布到 Kubernetes 中。

```yaml
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
 name: deploy-using-kubectl
spec:
 inputs:
 resources:
 - name: git-source
   type: git
    params:
 - name: pathToYamlFile
   description: The path to the yaml file to deploy within the git source
 - name: imageUrl
   description: Url of image repository
 - name: imageTag
   description: Tag of the images to be used.
    default: "latest"
    steps:
 - name: update-yaml
   image: alpine
    command: ["sed"]
    args:
 - "-i"
 - "-e"
 - "s;__IMAGE__;$(inputs.params.imageUrl):$(inputs.params.imageTag);g"
 - "/workspace/git-source/$(inputs.params.pathToYamlFile)"
 - name: run-kubectl
   image: registry.cn-hangzhou.aliyuncs.com/knative-sample/kubectl:v0.5.0
    command: ["kubectl"]
    args:
 - "apply"
 - "-f"
 - "/workspace/git-source/$(inputs.params.pathToYamlFile)"
```

### 定义 Pipeline

现在我们已经有两个 Task 了，现在我们就用一个 PIpeline 来编排这两个Task：

```yaml
apiVersion: tekton.dev/v1alpha1
kind: Pipeline
metadata:
 name: build-and-deploy-pipeline
spec:
 resources:
 - name: git-source
   type: git
    params:
 - name: pathToContext
   description: The path to the build context, used by Kaniko - within the 
   workspace
    default: src
 - name: pathToYamlFile
   description: The path to the yaml file to deploy within the git source
 - name: imageUrl
   description: Url of image repository
 - name: imageTag
   description: Tag to apply to the built image
    tasks:
 - name: source-to-image
   taskRef:
    name: source-to-image
    params:
 - name: pathToContext
   value: "$(params.pathToContext)"
 - name: imageUrl
   value: "$(params.imageUrl)"
 - name: imageTag
    value: "$(params.imageTag)"
    resources:
    inputs:
 - name: git-source
   resource: git-source
 - name: deploy-to-cluster
   taskRef:
    name: deploy-using-kubectl
    runAfter:
 - source-to-image
   params:
 - name: pathToYamlFile
   value: "$(params.pathToYamlFile)"
 - name: imageUrl
   value: "$(params.imageUrl)"
 - name: imageTag
   value: "$(params.imageTag)"
    resources:
    inputs:
 - name: git-source
   resource: git-source
```

### 鉴权信息

如下所示，定义一个 Secret 和 ServiceAccount。并且给 ServiceAccount 绑定执行 Knative Service 的权限。首先创建一个 Secret 保存镜像仓库的鉴权信息，如下所示 :

+ tekton.dev/docker-0 换成你要推送的镜像仓库的地址
+ username 换成镜像仓库鉴权的用户名
+ password 环境镜像仓库鉴权的密码

```yaml
  apiVersion: v1
  kind: Secret
  metadata:
   name: ack-cr-push-secret
   annotations:
   tekton.dev/docker-0: https://registry.cn-hangzhou.aliyuncs.com
  type: kubernetes.io/basic-auth
  stringData:
   username: <cleartext non-encoded>
   password: <cleartext non-encoded>
```


  下面这些信息保存到文件中，然后使用 kubectl apply -f 指令提交到 Kubernetes。

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
 name: pipeline-account
secrets:
- name: ack-cr-push-secret
---
apiVersion: v1
kind: Secret
metadata:
 name: kube-api-secret
 annotations:
 kubernetes.io/service-account.name: pipeline-account
type: kubernetes.io/service-account-token
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: pipeline-role
rules:
- apiGroups: ["serving.knative.dev"]
 resources: ["services"]
 verbs: ["get", "create", "update", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
 name: pipeline-role-binding
roleRef:
 apiGroup: rbac.authorization.k8s.io
 kind: Role
 name: pipeline-role
subjects:
- kind: ServiceAccount
 name: pipeline-account
```


### 定义 PIpelineRun

ServiceAccount 对应的鉴权信息是通过和 PIpelineRun 绑定的方式执行的。参见 run/picalc-pipeline-run.yaml 文件。

```yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineRun
metadata:
 generateName: tekton-kn-sample￾spec:
 pipelineRef:
 name: build-and-deploy-pipeline
 resources:
 - name: git-source
   resourceRef:
    name: tekton-knative-git
    params:
 - name: pathToContext
   value: "src"
 - name: pathToYamlFile
   value: "knative/helloworld-go.yaml"
 - name: imageUrl
   value: “registry.cn-hangzhou.aliyuncs.com/knative-sample/tekton￾knative-helloworld”
 - name: imageTag
   value: "1.0"
    serviceAccount: pipeline-account
```

### 运行 Tekton HelloWorld

准备 PIpeline 的资源。

```shell
kubectl apply -f tasks/source-to-image.yaml -f tasks/deploy-using-kubectl.
yaml -f resources/picalc-git.yaml -f image-secret.yaml -f pipeline-account.
yaml -f pipeline/build-and-deploy-pipeline.yaml
```

执行 create 把 pipelieRun 提交到 Kubernetes 集群。之所以这里使用 create而不是使用 apply 是因为PIpelineRun 每次都会创建一个新的，kubectl 的 create指令会基于 generateName 创建新的 PIpelineRun 资源。

```shell
kubectl create -f run/picalc-pipeline-run.yaml
```

查看一下 pod 信息可能是下面这样：

```shell
# kubectl get pod
NAME READY STATUS RESTARTS AGE
tekton-kn-sample-45d84-deploy-to-cluster-wfrzx-pod-f093ef 0/3 Completed 
0 8h
tekton-kn-sample-45d84-source-to-image-7zpqn-pod-c2d20c 0/2 Completed 
0 8h
```

此时查看 Knative service 的配置：

```shell
# kubectl get ksvc
NAME URL 
LATESTCREATED LATESTREADY 
READY REASON
tekton-helloworld-go http://tekton-helloworld-go.default.knative.
kuberun.com tekton-helloworld-go-ntksb tektonhelloworld-go-ntksb True
```

通过浏览器访问 http://tekton-helloworld-go.default.knative.kuberun.com可以看到 hello World。



